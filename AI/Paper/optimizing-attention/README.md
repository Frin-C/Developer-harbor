# 论文复现：《Optimizing Attention》

## 论文介绍

这篇文章提出了一种新的优化方法来替代传统的注意力机制，通过解决优化问题实现注意力功能，无需可训练矩阵，可降低存储和计算成本，提高模型性能。本文发表于[NeurIPS 2024 Workshop - OPT 2024: Optimization for Machine Learning](https://openreview.net/forum?id=HQvVSOdflM)，作者Hanno Ackermann、Hong Cai、Markus Nagel、Leyla Mirvakhabova、Farhad G. Zanjani和Fatih Porikli.

## 模板结构
```
optimizing-attention/   # 项目根目录
├── README.md           # 项目说明文档
├── requirements.txt    # 依赖包列表
└── experiments/         # 实验代码及数据相关
```

## 使用方法
1. 安装依赖：

   - 切换工作目录到experiments

      ```bash
      cd path/to/project/AI/Paper/optimizing-attention/experiments/
      ```

   - 创建conda环境

      ```bash
      conda create -n optimizing-attention python==3.8.10 -y
      conda activate optimizing-attention
      ```

   - 安装子模块lqp_py用于admm求解器

      ```bash
      git submodule init .
      git submodule update . 
      
      cd admm
      pip install -e . 
      cd ..
      ```

   - 安装python依赖库

      ```bash
      pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121
      pip install einops line_profiler icecream tqdm
      ```
   
   - 安装kaiwu库

      从[QBoson平台](https://platform.qboson.com/sdkDownload)下载安装kaiwuSDK并解压。

      ```bash
      pip install kaiwu-1.1.2-py3-none-any.whl 
      ```

2. 训练示例：

   ```bash
   python train.py
   ```

3. 模拟退火推理示例：

   ```bash
   python eval.py
   ```

4. CIM光量子计算机推理实例

   ```bash
   cd cim
   python eval_0.py
   tar -cvzf mat.tar.gz mat
   ```

   下载打包好的CSV矩阵文件，并上传到[QBoson云平台](https://platform.qboson.com/)使用相干光量子计算机进行计算。

   运行结束后，得到打包好的报告文件和结果文件，可以使用rename.py文件进行重命名。

   ```bash
   python eval_1.py
   cd ..
   ```

## 研究背景

### 经典注意力机制的核心问题

- 参数量大：传统注意力机制依赖可训练的QKV矩阵（ $W_q, W_k, W_v$ ），这些矩阵占模型参数的大部分，导致内存和带宽需求高，尤其在移动端GPU上受限于缓存和内存带宽瓶颈。
  
- 计算复杂度高：注意力矩阵的计算（如Softmax）具有 $O(N^2)$ 的时间和空间复杂度，限制了长序列处理能力。

- 稀疏性不足：传统Softmax生成的注意力权重通常是密集的，导致计算冗余，无法有效利用数据的稀疏性。

### 本文解决的问题

- 降低参数量：通过去除可学习的参数矩阵，以及去除Key矩阵，保留Query和Value矩阵来构建目标函数，从而达到降低参数量的作用，对于Cross-Attention有利。

- 线性化注意力得分求解：将注意力得分的计算从softmax非线性的操作转换为5次方的线性操作。

- 稀疏性惩罚：通过对目标函数施加惩罚项，从而增强系数的稀疏性。

- 最优化问题的反向传播：通过ADMM算法来实现在深度学习层中插入最优化方法的反向传播。这种方法依赖于SVD和反复迭代。

## 算法说明

### 原文经典算法

原文注意力机制通过优化问题重构注意力，目标是最小化Query与Value线性组合的误差，同时引入L1正则化约束稀疏性：  


$$
\min_{x_i} \| q_i^T - x_i^T V \|^2_2 + \lambda \| x_i \|_1
$$


目标函数构成：  
  
- 数据拟合项： $\| q_i^T - x_i^T V \|^2_2$ 衡量Value线性组合与Query的逼近程度。
  
- 稀疏性约束： $\lambda \| x_i \|_1$ 强制大部分 $x_i$ 为0，减少冗余计算。  
  
求解方法：通过ADMM（Alternating Direction Method of Multipliers）迭代更新变量 $x,z, \mu$ ，逐步逼近最优解。  

### 复现构建为离散QUBO模型

将连续系数 $x_j \in \mathbb{R}$ 替换为二进制变量 $x_j \in \{0, 1\}$ ，表示是否选择第 $j$ 个Value向量参与重建Query。目标函数转换为QUBO的标准形式：


$$
H = x^T J x + x h + C
$$


其中：  
- $J$ 为二次项系数矩阵。  
- $h$ 为一次项系数向量。  
- $C$ 为常数项（可省略）。  

展开平方项：


$$
\| q_i^T - x_i^T V \|^2_2 = (q_i - x_i^T V)(q_i - x_i^T V)^T = q_i^T q_i - 2 q_i V^T x_i + x_i^T V V^T x_i
$$


- 常数项： $q_i^T q_i$ （不影响优化结果，可忽略）
- 一次项： $-2 q_i V^T x_i$ 。 
- 二次项： $x_i^T V V^T x_i$ 
 

处理L1正则项：


$$
\lambda \| x_i \|_1 = \lambda x_i^T \cdot I^{m \times 1}
$$
-  $I^{m \times 1}$ 是全1向量。  


合并目标函数（忽略常数项并简化目标函数）：


$$
H = x_i^T V V^T x_i - 2 q_i V^T x_i + \lambda x_i^T \cdot I^{m \times 1}
$$


转换为QUBO标准形式：  


$$
H = \frac{1}{2} x_i^T J x_i + x_i h + C
$$


- $J = 2 V V^T$ ：二次项系数矩阵。  
- $h = -2 q_i V^T$ ：一次项系数向量。  
- $C = 0$ ：常数项（可省略）。  

其中：
- Query矩阵 $Q$ ：形状为 $n\times d$ ，Query矩阵由 $n$ 个 $d$ 维的特征向量构成，其中 $q_i$ 表示第 $i$ 个Query向量。

- Value矩阵 $V'$ ：形状为 $m\times d$ ，Value矩阵由 $m$ 个 $d$ 维的特征向量构成。注意在实现时，以往的实数系数会保持系数与V的加权求和接近于 $q_i$ ，而系数被离散化到01会导致Q和V之间匹配的失衡（只会有一个 $V_i$ 被选择与 $q_i$ 匹配，因为V的特征向量个数是 $q_i$ 的 $m$ 倍），因此需要对V进行缩放以进行平衡，缩放通过 $V'=V/m$ 实现。

- 系数向量 $x$ ：长度 $m$ ：每个元素 $x_j \in \{0, 1\}$ 表示是否选择第 $j$ 个Value向量与 $q_i$ 做匹配。因此总共需要 $n\times m$ 个量子比特来整个目标函数。

- 正则化系数 $\lambda$ ：控制稀疏性，需根据具体任务调整。 

结果处理 ：

- 归一化：对CIM输出的二进制系数 $x$ 进行归一化（5次方归一化），生成注意力权重。  
- 加权聚合：加权聚合Value矩阵，输出最终结果。   
  

### 复现流程

<img src="./images/model.png" width="500" height="400">

原文采用的实验模型过于庞大和复杂，在复现为QUBO模型时采用简易的Attention模块做特征提取和MLP模块做分类头。如上图所示展现了用于实验的模型的结构图，在训练的过程中，采用ADMM求解器来进行可导的训练，在推理的过程中，可以使用kaiwuSDK提供的模拟退火算法或CIM量子计算真机实现。

在目标函数的构建上，我们发现该目标函数实际上是由 $n$ 个子目标函数构成的，每个子目标函数考察 $q_i$ 和 $V$ 之间的关系，因此子目标函数是彼此独立的，这在实现上减少了每次迭代所需的量子比特数，但增加了迭代数。将 $n*m$ 个比特数的一次性迭代降低至了 $m$ 个比特数的 $n$ 次迭代。

## 实验结果

### 模型训练

1. 实验配置:

- 数据集：MNIST数据集中的70000张图片，按照6：1划分训练集和测试集

- 实验设备：NVIDIA GeForce RTX 4090 



- 训练参数：

<div align="center">

| **参数名**         | **值**             |
|:-------------------:|:-------------------:|
| num_epoch          | 50                 |
| batch_size         | 128                |
| learning_rate      | 0.001              |
| optimizer          | Adam               |
| loss_function      | CrossEntropyLoss   |

</div>

- 模型参数配置：

<div align="center">

| **参数名**     | **值**            |
|:--------------:|:----------------:|
| image_size     | (28, 28)         |
| patch_size     | (4, 4)           |
| num_classes    | 10               |
| dim            | 64               |
| channels       | 1                |


</div>

2. 实验性能展示及与原始注意力模型对比结果：
   
   a) optimizing-attention模型训练的准确率和损失曲线图像如下：

   <img src="./images/model_acc_loss.png" width="1000" height="500">

   b) original-attention模型训练的准确率和损失曲线图像如下：

   <img src="./images/original_attn_a_l.png" width="1000" height="500">

3. 结果分析:

   原始注意力模型在MNIST数据集上表现优异，准确率达到92.1%。而模型训练的结果表明optimizing-attention模型在MNIST数据集上表现良好，损失迅速下降，且准确率仅达到82.7%。由此说明，optimizing-attention模型在图像分类准确率上损失较小。

   基于optimizing attention机制的连续权重版本准确率额达到89.8%，与原版注意力模型差异较小。考虑到比特数限制，optimizing attention经过二值化QUBO处理后，准确率降低了7%左右，差异较小。


### 仿真实验对比分析
1. 仿真实验对比结果

   我们通过在optimizing-attention模型上进行admm求解器和kaiwu模拟退火求解器的仿真对比实验，根据不同label计算置信度，从而绘制了一幅箱线图，如下所示：

   <img src="./images/boxplot_sa_vs_admm.png" width="1000" height="500">

2. 结果分析：

   a) 对于同一个标签，大多数情况下optimizing-attention模型在admm求解器与kaiwu模拟退火表现相当。仅在数字为3时，admm求解器标签的置信度结果存在更大的波动。这说明kaiwu模拟退火求解器在求解时更加稳定。

   b) 对于不同的标签，模型在判别图像为0，1，4，6时具有较高的中位数和较窄的四分位距，这说明模型在判别以上数字时具有相当高的置信度，而对于数字5存在比较严重的误判情况，因为此时的中位数较低且分布范围广。该结论对两种求解器均成立。这说明该模型对于数字5的判别效果较差。



### QBoson CPQC-1 CIM 量子退火真机推理
1. 我们通过在QBoson CPQC-1 CIM量子退火真机上进行推理实验，得到了49次运行结果并作表如下，表中展示实验的任务编号、QUBO值和运行时间。以下任务的平均运行时间为8.1537ms，体现出QBoson CPQC-1 CIM具有优异的运行效率。

| Task_idx | QUBO_Value | Runtime(ms) | | Task_idx | QUBO_Value | Runtime(ms) | | Task_idx | QUBO_Value | Runtime(ms) |
|----------|------------|-------------|------------|----------|------------|-------------|------------|----------|------------|-------------|
| 0        | -1176      | 5.359       | | 16       | -1428      | 4.575       | | 32       | -1248      | 5.553       |
| 1        | -1076      | 4.718       | | 17       | -876       | 5.209       | | 33       | -1052      | 6.691       |
| 2        | -2968      | 7.174       | | 18       | -3808      | 5.488       | | 34       | -884       | 4.398       |
| 3        | -1344      | 11.196      | | 19       | -4544      | 2.846       | | 35       | -1408      | 9.564       |
| 4        | -1544      | 1.783       | | 20       | -2228      | 6.267       | | 36       | -3204      | 6.632       |
| 5        | -788       | 0.663       | | 21       | -952       | 9.177       | | 37       | -3316      | 16.968      |
| 6        | -1500      | 0.181       | | 22       | -664       | 4.544       | | 38       | -876       | 3.528       |
| 7        | -1892      | 9.435       | | 23       | -4072      | 18.783      | | 39       | -1228      | 6.065       |
| 8        | -1516      | 0.682       | | 24       | -1584      | 10.713      | | 40       | -804       | 7.555       |
| 9        | -3736      | 5.767       | | 25       | -748       | 15.859      | | 41       | -2192      | 10.837      |
| 10       | -1484      | 8.141       | | 26       | -3016      | 7.25        | | 42       | -1368      | 21.37       |
| 11       | -2076      | 8.452       | | 27       | -740       | 2.368       | | 43       | -1304      | 0.134       |
| 12       | -3252      | 9.94        | | 28       | -2108      | 15.446      | | 44       | -1792      | 17.131      |
| 13       | -1004      | 7.615       | | 29       | -1092      | 4.989       | | 45       | -1564      | 13.729      |
| 14       | -652       | 10.439      | | 30       | -772       | 0.811       | | 46       | -1152      | 10.75       |
| 15       | -752       | 9.865       | | 31       | -2384      | 9.371       | | 47       | -1308      | 19.794      |

2. 下表展示对于任务22的真机预测结果和置信度分析，可以体现cim真机极高的预测准确率。

| idx | label | prediction | correct | logit_0 | logit_1 | logit_2 | logit_3 | logit_4 | logit_5 | logit_6 | logit_7 | logit_8 | logit_9 |
|-----|-------|------------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|
| 22  | 6     | 6          | True    | 2.175622057620785e-06 | 9.58766885328366e-11  | 4.8994350223143545e-18 | 1.5787473011440877e-18 | 0.001527638640254736 | 2.971241308546979e-12 | 0.99847012758255 | 1.5799543007233297e-08 | 1.7620209735724757e-09 | 3.695603822961857e-09 |
   
### 稀疏性分析

对optimizing-attention的稀疏能力进行可视化。下图中共有49(7×7)个大方块，大方块内部又被划分为49(7×7)个小方块，大小方块分别对应于Query向量和Value向量，我们希望通过这种稀疏性热力图来展示optimizing-attention的稀疏能力。图中紫色代表0，黄色代表1。可以看到optimizing-attention的每个Q只需要一部分V来匹配，而非原始注意力模型的全匹配。这说明optimizing-attention模型具有较高的稀疏性。

<img src="./images/OptimizingAttentionHeatmaps.png" width="1000" height="1000">

## 总结讨论

本部分主要说明一些在复现过程中的总结。

**原文创新点：**

1. 去除矩阵K，减少参数量：大胆地去除了矩阵K的存在，通过找到Q的分量与V的分量的相似性系数来作为注意力
2. 转换为最优化问题，替代softmax机制：将注意力得分的计算问题转换为一个最优化问题，以替代传统的softmax机制。这使得可以在局部添加稀疏性惩罚项，而非像以往一样在训练目标函数里全局添加惩罚。

**复现为QUBO模型：**

1. 实现与CIM相干光量子计算的结合：为了更好地与CIM相干光量子计算进行结合实践，将原文问题转换为QUBO问题，有利于在量子退火计算上的实现。
2. 处理系数变量离散化问题：对系数变量进行离散化处理，尽管这会带来一定的精度损失和值域缩放问题，但通过动态缩放等方法解决了这些问题，验证了转换为QUBO模型后的表达效果。
3. 保持稀疏性优势：复现后的模型保留了原文方法的稀疏性优势，通过稀疏性热力图展示了其较高的稀疏性。

**原文对于CIM的应用优势：**

1. 自然过渡到QUBO：原文已经将注意力机制的问题转换为最优化问题，因此可以很自然地过渡到QUBO模型，为在CIM上进行量子退火计算提供了便利。
2. 提供实践指导：尽管存在实数系数与离散变量之间的转换问题，但总体上该方法仍具有一定的指导性作用，为将注意力机制与量子计算相结合提供了一种可行的思路。

**本方法的不足：**

1. 计算量较大：该方法需要计算每个Q和V之间的关系，尽管引入了稀疏性惩罚，但总的计算量并未减少，只是缩小了注意力的视野范围。例如在实验中，对于一个样本，将图片分割为49块，就需要跑49次CPQC-1（49qubit）。
2. 不利于深度学习大模型：一方面，由于本方法去除了参数，因此在堆叠参数时需要用其他层来堆叠，无法做到即插即用；另一方面，本方法不适合堆叠的层次结构，因为经过一层一层的OptimizingAttention后，Q和V会朝着越来越相似的方向发展，导致层数越多，意义越小。

**展望：**

1. 更适合Cross-Attention场景：相较于使用Self-Attention的模型，本方法更加适合于Cross-Attention的场景，能够在Cross-Attention中发挥更大的优势。
2. 离散化系数改进为Onehot编码，增加离散化系数细分程度，减小离散化为二值带来的效果损失。

**附加问题：**

**Q1:** 论文中将attention机制中的softmax结果替换成优化变量，用V的加权值和Q的近似程度来衡量权重的好坏。这个思路为什么是合理的，为什么能work，加权之后和Q尽量接近，那直接用Q不行吗，叙述下对这个方案的理解。
   
A: attention是通过动态组合V来表达Q。用最优化方法做attention，本质上是“反向构造”——找到一组权重使得Q能被V解释或逼近，这种方式逻辑上和softmax有异曲同工之妙。

为什么能work：

- 传统 attention 中，softmax 本质上是给定一组相似度（比如 $QK^T$），通过指数函数和归一化选出相对重要的 $K$。

- 而优化方法则是直接从目标出发：你希望什么样的 Value 组合最接近给定的 Query，这比用 softmax 间接建模相似性更加明确。

为什么不能直接用Q：

- 如果直接用Q，就失去了注意力的“查询-匹配”机制，本质上你只是拿Query去走一个线性层，不再有动态权重加权，这和传统MLP没有区别。

- 而attention的精髓正是在于，给定Q后，根据上下文（也就是V）动态生成表示。


**Q2:** 基于优化的Attention模型准确率82.7%，有没有试过去掉这个基于优化的attention，直接用剩下的全连接层，能到多少准确率?

对注意力机制部分进行替换，替换为MLP做特征提取，其余部分不变。测试得到准确率72.1%。补充实验代码及参数位于only_mlp目录下。

- 替换attention为优化attention能显著优于纯MLP（+10.6%），说明attention的结构仍起到关键作用，即使没有传统的softmax。

- 但优化attention仍不如传统attention，说明softmax机制和可训练的QKV矩阵在表达能力上仍有优势。

这验证了：优化型attention是一种在去参数、增强稀疏性和结构简化条件下的有效近似，但并非性能最优。

**Q3:** 表示正则项的时候，用权重乘上全1向量，是否正确，请给出一些解释说明。使用二范数或不等式约束形式如何。

A: 回答部分一：这种做法是数学上合理且标准的表示。

正则项公式：


```math
\lambda \| x_i \|_1 = \lambda \sum_j |x_{ij}| = \lambda x_i^T \cdot 1^{m \times 1}
```


其中 $\mathbf{1}$ 是全 1 向量，与 $x_i$ 同维度。

- 对于 $x_i \in {0, 1}$ 的情况（QUBO 离散化后）， $|x_{ij}| = x_{ij}$ ，所以 $L_1$ 范数就是所有元素的和。

- 计算上， $\lambda x_i^T \cdot \mathbf{1}$ 就是 $\lambda$ 乘以激活变量的数量（即选了多少个 $V_j$）。

- 在代码或公式中这种向量乘法的写法，能更好地和矩阵表示兼容，易于实现。

因此，用 $\lambda x_i^T \cdot \mathbf{1}$ 表示正则项是正确且简洁的形式，等价于传统的 $L_1$ 正则表示。

回答部分二：QUBO变量 $x$ 只能取0或1，对于 $L_1$ 或 $L_2$ 范数来说一样的，因为 $x^2=x$。

如果通过不等式约束来实现，即引入辅助变量 $t_i$， 是优化中经典的做法，将不可微的L1范数问题转化为线性目标+线性约束：


$$
\min_{x,t} \| q-x^TV \|^2_2 + \lambda \sum_j t_j, \quad s.t. \quad -x_j \le t_j \le x_j
$$


优点：

- 保留了L1正则的稀疏特性；

- 可用于凸优化框架（如LP或QP）；

- 在实值优化变量时尤其有效。

缺点：

- 会引入额外变量和约束，增加求解复杂度；这样子做会更加的逼近与实数权重，但是会造成极其严重的qubit消耗，假设 $-2^n \le t_j \le 2^n$，就会造成 $n$ 倍的qubit消耗，这对于当前量子比特较少的量子退火机是难以接受和实现的。


## 作者信息
- 作者姓名：杜鹏(du_peng@csu.edu.cn)，李世成(lishic@csu.edu.cn)，王硕蕾(wangs.l@csu.edu.cn)
