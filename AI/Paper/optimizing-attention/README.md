# 论文复现项目模板

## 模板简介
这是一个通用的论文复现项目模板，适用于各种领域的论文复现工作。本项目遵循量子开发实验室的代码规范和项目结构要求。

## 模板结构
```
paper-demo/              # 项目模板目录
├── README.md           # 项目说明文档
├── requirements.txt    # 依赖包列表
└── demo.py            # 主程序入口
```

## 使用方法
1. 安装依赖：

   - 切换工作目录到experiments

      ```bash
      cd path/to/project/AI/Paper/optimizing-attention/experiments/
      ```

   - 创建conda环境

      ```bash
      conda create -n optimizing-attention python==3.8.10 -y
      conda activate optimizing-attention
      ```

   - 安装子模块lqp_py用于admm求解器

      ```bash
      git submodule init .
      git submodule update . 
      
      cd admm
      pip install -e . 
      cd ..
      ```

   - 安装python依赖库

      ```bash
      pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121
      pip install einops line_profiler icecream tqdm
      ```
   
   - 安装kaiwu库

      从[QBoson平台](https://platform.qboson.com/sdkDownload)下载安装kaiwuSDK并解压。

      ```bash
      pip install kaiwu-1.1.2-py3-none-any.whl 
      ```



2. 训练示例：

   ```bash
   python train.py
   ```

3. 模拟退火推理示例：

   ```bash
   python eval.py
   ```

4. CIM光量子计算机推理实例

   ```bash
   cd cim
   python eval_0.py
   tar -cvzf mat.tar.gz mat
   ```

   下载打包好的CSV矩阵文件，并上传到[QBoson云平台](https://platform.qboson.com/)使用相干光量子计算机进行计算。

   运行结束后，得到打包好的报告文件和结果文件，可以使用rename.py文件进行重命名。

   ```bash
   python eval_1.py
   cd ..
   ```

## 研究背景

### 经典注意力机制的核心问题
- **参数量大**：  
  传统注意力机制依赖可训练的QKV矩阵（$W_q, W_k, W_v$），这些矩阵占模型参数的大部分，导致内存和带宽需求高，尤其在移动端GPU上受限于缓存和内存带宽瓶颈。
  
- **计算复杂度高**：  
  注意力矩阵的计算（如Softmax）具有 $O(N^2)$ 的时间和空间复杂度，限制了长序列处理能力。

- **稀疏性不足**：  
  传统Softmax生成的注意力权重通常是密集的，导致计算冗余，无法有效利用数据的稀疏性。

- **训练与推理的瓶颈**：  
  ADMM等优化算法在反向传播中需多次迭代，计算开销大，且依赖硬件资源（如GPU），难以在资源受限设备上高效运行。

### QUBO与CIM的改进
- **离散化稀疏性**：  
  将连续的稀疏系数优化问题转化为二进制变量（0/1）的QUBO问题，通过CIM（相干异步计算）的并行计算能力加速求解。

- **内存与计算效率**：  
  通过QUBO建模减少对QKV矩阵的依赖，直接利用原始输入标记作为Query和Value，降低内存占用。

- **硬件适配性**：  
  CIM的并行性和低功耗特性使其更适合移动端或嵌入式设备，突破传统GPU的缓存和带宽限制。


## 算法说明
<span style="color:red">这里写详细的算法原理(WSL)</span>

**经典目标函数**：
传统注意力机制通过优化问题重构注意力，目标是最小化Query与Value线性组合的误差，同时引入L1正则化约束稀疏性：  
$$
\min_{x_i} \| q_i^T - x_i^T V \|^2_2 + \lambda \| x_i \|_1
$$  
- **目标函数构成**：  
   **数据拟合项**：$ \| q_i^T - x_i^T V \|^2_2 $ 衡量Value线性组合与Query的逼近程度。  
   **稀疏性约束**：$ \lambda \| x_i \|_1 $ 强制大部分 $ x_i $ 为0，减少冗余计算。  
- **求解方法**：通过ADMM（Alternating Direction Method of Multipliers）迭代更新变量 $ x, z, \mu $，逐步逼近最优解。  

 
**离散化变量**： 
将连续系数 $ x_j \in \mathbb{R} $ 替换为二进制变量 $ x_j \in \{0, 1\} $，表示是否选择第 $ j $ 个Value向量参与重建Query。目标函数转换为QUBO的标准形式：  
$$
H = x^T J x + x h + C
$$  
其中：  
- $ J $ 为二次项系数矩阵。  
- $ h $ 为一次项系数向量。  
- $ C $ 为常数项（可省略）。  

**展开平方项并处理L1正则化项**：

$$
\| q_i^T - x_i^T V \|^2_2 = (q_i - x_i^T V)(q_i - x_i^T V)^T = q_i^T q_i - 2 q_i V^T x_i + x_i^T V V^T x_i
$$  
- **常数项**：$ q_i^T q_i $（不影响优化结果，可忽略）。  
- **一次项**：$ -2 q_i V^T x_i $。  
- **二次项**：$ x_i^T V V^T x_i $。  
 
$$
\lambda \| x_i \|_1 = \lambda x_i^T \cdot I^{m \times 1}
$$  
- $ I^{m \times 1} $ 是单位向量（所有元素为1）。  

**合并目标函数**：
忽略常数项后，目标函数简化为：  
$$
H = x_i^T V V^T x_i - 2 q_i V^T x_i + \lambda x_i^T \cdot I^{m \times 1}
$$  
- **QUBO标准形式**：  
  $$
  H = \frac{1}{2} x_i^T J x_i + x_i h + C
  $$  
  $ J = 2 V V^T $：二次项系数矩阵。  
  $ h = -2 q_i V^T $：一次项系数向量。  
  $ C = 0 $：常数项（可省略）。  

其中：
- **Query矩阵 $ Q $**：行数 $ n $是Query向量的数量，列数 $ d $：每个Query向量的维度，$ q_i $：第 $ i $ 个Query向量。

- **Value矩阵 $ V' $**：行数 $ m $是Value向量的数量，列数 $ d $是每个Value向量的维度。$ V' = V/m $ 通过缩放 $ V $ 的值以平衡目标函数的数值范围。

- **稀疏系数向量 $ x $**：长度 $ m $：每个元素 $ x_j \in \{0, 1\} $ 表示是否选择第 $ j $ 个Value向量。

- **正则化系数 $ \lambda $**：控制稀疏性，需根据具体任务调整。 

**CIM求解**：
将QUBO问题 $ H = x^T J x + x h + C $ 映射到CIM（Coherent Ising Machine）的物理系统，利用其并行性快速找到近似最优解。  
求解过程：  
1. **初始化**：设置初始的二进制变量 $ x $。  
2. **迭代更新**：通过CIM的物理动态过程（如能量最小化）寻找最优解。  
3. **输出结果**：得到二进制变量 $ x $ 的最优组合，表示选择的Value向量。 
   
结果处理 ：
- **归一化**：对CIM输出的二进制系数 $ x $ 进行归一化（如5次方归一化），生成注意力权重。  
- **加权聚合**：加权聚合Value矩阵，输出最终结果。   
  

**此外，为解决数值稳定性和优化效率问题，这里还进行了关键处理**：
1. **缩放矩阵 $ V $ 和 $ \lambda $**：  
    $ V' = V/m $：减少二次项 $ V V^T $ 的数值范围，避免计算溢出。
    $ \lambda x_j \cdot I^{m \times 1}/m $：缩放正则化项，使其与目标函数其他部分的尺度一致。

2. **避免矩阵退化**：  
    **问题**：Value矩阵 $ V $ 可能退化（如秩不足），导致 $ V V^T $ 不可逆。
    **解决方案**：  
      向 $ V $ 中添加随机Token（服从 $ N(0,1) $ 的噪声向量），增加矩阵的秩。
      在反向传播中对 $ V^T V $ 的对角线添加均匀分布噪声 $ r \sim U(0, \sigma) $，防止梯度爆炸。

3. **L1范数的离散化等效**：  
    **问题**：L1范数 $ \| x \|_1 $ 在离散化后无法直接计算。
    **解决方案**：  
      使用 $ x^T \cdot I^{m \times 1} $ 近似L1范数。
      引入辅助变量 $ t $，通过约束 $ x_j \leq t_j $ 和 $ t_j \leq M x_j $（$ M $ 为大常数）来等效L1范数，但会增加计算复杂度。



- 实现细节

   <span style="color:red">这里写算法实现的流程，模型流程(DP)</span>



- 关键步骤

   <span style="color:blue">这里简要讲解整个文章的流程，分条列举文章的优势，为什么是优势(WSL)</span>

**量子算法流程**：
1. **输入处理**：  
   对输入序列进行预处理，提取Query矩阵 $Q$ 和Value矩阵 $V$。在Self-Attention中， $Q$ 和 $V$ 共享同一输入；在Cross-Attention中， $Q$ 和 $V$ 来自不同输入。
  
2. **QUBO建模**  
   将每个Query $q_i$ 的重构问题转化为QUBO形式：
     $$
     L_i = \min_{x} \| q_i - x^T V' \|^2_2 + \lambda x^T \cdot I'
     $$
   对 $V$ 和 $\lambda$ 进行缩放（如 $V' = V/m$）以平衡目标函数。

3. **CIM求解**：  
   将QUBO问题映射到CIM的物理系统（如光子网络或超导量子器件），利用CIM的并行性和低能耗特性快速求解二进制变量 $x_j$ 的最优组合。
   
4. **结果处理**：  
   对CIM输出的二进制系数 $x_j$ 进行归一化（如5次方归一化），生成注意力权重，加权聚合Value矩阵，输出最终结果。
   
5. **输出生成**：  
   将稀疏注意力权重应用于后续任务（如分类或检测），完成模型推理。


**量子算法的核心优势**：
| **优势** | **为什么是优势** |
|----------|------------------|
| **高效处理稀疏性** | <ul><li>传统ADMM需迭代求解稀疏系数，而QUBO直接通过二进制变量约束稀疏性，避免冗余计算。</li><li>CIM的并行性可同时处理多个Query的QUBO问题，显著提升稀疏性优化效率。</li></ul> |
| **内存与计算效率** | <ul><li>无需存储QKV矩阵，直接利用原始输入数据，减少内存占用。</li><li>CIM的硬件加速能力降低计算延迟，适合移动端低功耗场景。</li></ul> |
| **突破传统优化瓶颈** | <ul><li>ADMM的迭代过程在GPU上计算开销大，而CIM通过物理系统的并行求解避免迭代，加速收敛。</li><li>QUBO建模简化了目标函数的复杂性，减少反向传播中的梯度计算。</li></ul> |
| **硬件适配性** | <ul><li>CIM的低功耗和并行性使其适合部署在边缘设备（如手机、无人机），突破GPU的缓存和带宽限制。</li><li>量子硬件的专用性可针对QUBO问题进行定制化优化，提升能效比。</li></ul> |
| **可扩展性** | <ul><li>QUBO框架可灵活扩展到多头注意力或跨模态任务，适应不同规模的输入序列。</li><li>通过调整二进制变量的数量和QUBO参数，可平衡稀疏性与模型精度。</li></ul> |




- 实现细节

   <span style="color:red">这里写算法实现的流程，模型流程(DP)</span>

- 关键步骤

   <span style="color:blue">这里简要讲解整个文章的流程，分条列举文章的优势，为什么是优势(WSL)</span>

- 创新点

   <span style="color:blue">论文创新点(DP)</span>

### 性能分析

   <span style="color:blue">实验(LSC)</span>

#### optimizing-attention模型训练

1. 实验配置:

- 数据集：MNIST数据集中的70000张图片，按照6：1划分训练集和测试集

- 实验设备：NVIDIA GeForce RTX 4090 



- 训练参数：

<div align="center">

| **参数名**         | **值**             |
|:-------------------:|:-------------------:|
| num_epoch          | 50                 |
| batch_size         | 128                |
| learning_rate      | 0.001              |
| optimizer          | Adam               |
| loss_function      | CrossEntropyLoss   |

</div>

- 模型参数配置：

<div align="center">

| **参数名**     | **值**            |
|:--------------:|:----------------:|
| image_size     | (28, 28)         |
| patch_size     | (4, 4)           |
| num_classes    | 10               |
| dim            | 64               |
| channels       | 1                |


</div>

2. 实验性能展示及与原始注意力模型对比结果：
   
   a) optimizing-attention模型训练的准确率和损失曲线图像如下：

   <img src="./images/model_acc_loss.png" width="1000" height="500">

   b) original-attention模型训练的准确率和损失曲线图像如下：

   <img src="./images/original_attn_a_l.png" width="1000" height="500">

3. 结果分析:

   原始注意力模型在MNIST数据集上表现优异，准确率达到92.1%。而模型训练的结果表明optimizing-attention模型在MNIST数据集上表现良好，损失迅速下降，且准确率仅达到82.7%。由此说明，optimizing-attention模型在图像分类准确率上没有优势。




#### 仿真实验对比分析
1. 仿真实验对比结果

   我们通过在optimizing-attention模型上进行admm求解器和kaiwu模拟退火求解器的仿真对比实验，根据不同label计算置信度，从而绘制了一幅箱线图，如下所示：

   <img src="./images/boxplot_sa_vs_admm.png" width="1000" height="500">

2. 结果分析：

   a) 对于同一个标签，大多数情况下optimizing-attention模型在admm求解器与kaiwu模拟退火表现相当。仅在数字为3时，admm求解器标签的置信度结果存在更大的波动。这说明kaiwu模拟退火求解器在求解时更加稳定。

   b) 对于不同的标签，模型在判别图像为0，1，4，6时具有较高的中位数和较窄的四分位距，这说明模型在判别以上数字时具有相当高的置信度，而对于数字5存在比较严重的误判情况，因为此时的中位数较低且分布范围广。该结论对两种求解器均成立。这说明该模型对于数字5的判别效果较差。



#### QBoson CPQC-1 CIM 量子退火真机推理
1. 我们通过在QBoson CPQC-1 CIM量子退火真机上进行推理实验，得到了49次运行结果并作表如下，表中展示实验的任务编号、QUBO值和运行时间。以下任务的平均运行时间为8.1537ms，体现出QBoson CPQC-1 CIM具有优异的运行效率。

| Task_idx | QUBO_Value | Runtime(ms) | | Task_idx | QUBO_Value | Runtime(ms) | | Task_idx | QUBO_Value | Runtime(ms) |
|----------|------------|-------------|------------|----------|------------|-------------|------------|----------|------------|-------------|
| 0        | -1176      | 5.359       | | 16       | -1428      | 4.575       | | 32       | -1248      | 5.553       |
| 1        | -1076      | 4.718       | | 17       | -876       | 5.209       | | 33       | -1052      | 6.691       |
| 2        | -2968      | 7.174       | | 18       | -3808      | 5.488       | | 34       | -884       | 4.398       |
| 3        | -1344      | 11.196      | | 19       | -4544      | 2.846       | | 35       | -1408      | 9.564       |
| 4        | -1544      | 1.783       | | 20       | -2228      | 6.267       | | 36       | -3204      | 6.632       |
| 5        | -788       | 0.663       | | 21       | -952       | 9.177       | | 37       | -3316      | 16.968      |
| 6        | -1500      | 0.181       | | 22       | -664       | 4.544       | | 38       | -876       | 3.528       |
| 7        | -1892      | 9.435       | | 23       | -4072      | 18.783      | | 39       | -1228      | 6.065       |
| 8        | -1516      | 0.682       | | 24       | -1584      | 10.713      | | 40       | -804       | 7.555       |
| 9        | -3736      | 5.767       | | 25       | -748       | 15.859      | | 41       | -2192      | 10.837      |
| 10       | -1484      | 8.141       | | 26       | -3016      | 7.25        | | 42       | -1368      | 21.37       |
| 11       | -2076      | 8.452       | | 27       | -740       | 2.368       | | 43       | -1304      | 0.134       |
| 12       | -3252      | 9.94        | | 28       | -2108      | 15.446      | | 44       | -1792      | 17.131      |
| 13       | -1004      | 7.615       | | 29       | -1092      | 4.989       | | 45       | -1564      | 13.729      |
| 14       | -652       | 10.439      | | 30       | -772       | 0.811       | | 46       | -1152      | 10.75       |
| 15       | -752       | 9.865       | | 31       | -2384      | 9.371       | | 47       | -1308      | 19.794      |

2. 下表展示对于任务22的真机预测结果和置信度分析，可以体现cim真机极高的预测准确率。

| idx | label | prediction | correct | logit_0 | logit_1 | logit_2 | logit_3 | logit_4 | logit_5 | logit_6 | logit_7 | logit_8 | logit_9 |
|-----|-------|------------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|
| 22  | 6     | 6          | True    | 2.175622057620785e-06 | 9.58766885328366e-11  | 4.8994350223143545e-18 | 1.5787473011440877e-18 | 0.001527638640254736 | 2.971241308546979e-12 | 0.99847012758255 | 1.5799543007233297e-08 | 1.7620209735724757e-09 | 3.695603822961857e-09 |

3. 可视化optimizing-attention的稀疏能力
   下图中共有49(7×7)个大方块，大方块内部又被划分为49(7×7)个小方块，大小方块分别对应于Query向量和Value向量，我们希望通过这种稀疏性热力图来展示optimizing-attention的稀疏能力。图中紫色代表0，黄色代表1。可以看到optimizing-attention的每个Q只需要一部分V来匹配，而非原始注意力模型的全匹配。这说明optimizing-attention模型具有较高的稀疏性。

<img src="./images/OptimizingAttentionHeatmaps.png" width="1000" height="1000">

   
#### 稀疏性分析


- 总结讨论

   <span style="color:blue">各种各样乱七八糟的问题(DP)</span>

## 作者信息
- 作者姓名：[请填写作者姓名]